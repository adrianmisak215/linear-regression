{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Diagnostics for leverage and influence\n",
    "\n",
    "## 6.1 Importance of detecting influential observations\n",
    "\n",
    "\n",
    "<img src=\"images/6_1.png\" height=\"250px;\">\n",
    "\n",
    "Consider the situation illustrated in Figure 6.1. The point labeled A in this figure is remote in x space from the rest of the sample, but it lies almost on the regression line passing through the rest of the sample points. This is an example of a leverage point; that is, it has an unusual x value and may control certain model properties. Now this point does not affect the estimates of the regression coefficients, but it certainly will have a dramatic effect on the model summary statistics such as R2 and the standard errors of the regression coefficients. Now consider the point labeled A in Figure 6.2. This point has a moderately unusual x coordinate, and the y value is unusual as well. This is an influence point, that is, it has a noticeable impact on the model coefficients in that it “pulls” the regression model in its direction.\n",
    "\n",
    "We sometimes find that a small subset of the data exerts a disproportionate influence on the model coefficients and properties. In an extreme case, the parameter estimates may depend more on the influential subset of points than on the majority of the data. This is obviously an undesirable situation; we would like for a regression model to be representative of all of the sample observations, not an artifact of a few.\n",
    "\n",
    "In this chapter we present several diagnostics for leverage and influence. These diagnostics are available in most multiple regression computer packages. It is important to use these diagnostics in conjunction with the residual analysis techniques of Chapter 4. Sometimes we find that a regression coefficient may have a sign that does not make engineering or scientific sense, a regressor known to be important may be statistically insignificant, or a model that fits the data well and that is logical from an application–environment perspective may produce poor predictions. These situations may be the result of one or perhaps a few influential observations. Finding these observations then can shed considerable light on the problems with the model.\n",
    "\n",
    "## 6.2 Leverage\n",
    "\n",
    "As observed above, the location of points in x space is potentially important in determining the properties of the regression model. In particular, remote points potentially have disproportionate impact on the parameter estimates, standard errors, predicted values, and model summary statistics. The hat matrix $H=X(X'X)^{-1}X'$ plays an important role in identifying influential observations. As noted earlier, $H$ detemines the variances and covariances of $\\hat{y}$ and $e$, since $Var(\\hat{y}) = \\sigma^2H$, and $Var(e) = \\sigma^2(I-H)$. The elements $h_{ij}$ of the matrix $H$ may be interpreted as the amount of leverage exerted by the $i$-th observation $y_i$ on the $j$-th fitted value $\\hat{y}_j.$\n",
    "\n",
    "We focus on the diagonal elements:\n",
    "\n",
    "$$h_{ii} = x_i'(X'X)^{-1}x_i,$$\n",
    "\n",
    "where $x_i'$ is the $i$-th row of the $X$ matrix. The hat matrix diagonal is a standardized measure of the distance of the $i$th observation from the center of the $x$ space. Thus, large hat diagonals reveal observations that are potentially influential because they are remote in $x$ space from the rest of the sample. It turns out that the average size of a hat diagonal is $\\overline{h} = p / n$, and we traditionally assume that any observation for which the hat diagonal exceeds twice the average $2p/n$ is remote enough from the rest of the data to be considered a leverage point. \n",
    "\n",
    "Not all leverage points are going to be influential on the regresion coefficients. Because the hat diagonals examine only the location of the observation in $x$ space, some analysts like to look at the studentized resisudals or $R$-student in conjuction with the $h$_{ii}$. Observations with large hat diagonals and large residuals are likely to be influential. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Measures of influence: Cook's D\n",
    "\n",
    "In measuring influence, it is desirable to consider both the location of the point in the $x$ space, and the response variable. Cook has suggested a way to do this, using a measure of the squared distance between the least-squaes estimate based on all $n$ points $\\hat{b}$ and the estimate obtained by deleting the $i$-th point, say $\\hat{b}_{(i)}$. This can be expressed as:\n",
    "\n",
    "$$D_i = \\frac{(\\hat{b}_{(i)} - \\hat{b})M(b_{(i)} - \\hat{b})}{c}, \\quad i=1,...,n.$$\n",
    "\n",
    "The usual choices for $M, c$ are: $M=X'X, c=pMS_{Res}.$\n",
    "\n",
    "Points with large values of $D_i$ have consideable incfluence on the least-squares estimates $\\hat{b}$. The magnitude of $D_i$ is usually assessed by comparing it to $F_{\\alpha, p, n-p}$. If $D_i = F_{0.5, p, n-p}$, the deleting point $i$ would move $\\hat{b}_{(i)}$ top the boundary of an approximate 50% confidence region for $n$ based on the complete data set. That is a large displacement, and indicates that the least-squares estimate is sensitive to the ith data point. Since $F_{0.5, p, n-p} \\sim 1$, we usually consider points for which $D_i > 1$ to be influential. Ideally, we would like each estimate to stay within the boundary of 10 or 20% confidence region. \n",
    "\n",
    "The $D_i$ statistic can be rewritten as:\n",
    "$$D_i = \\frac{r_i^2}{p}\\frac{h_{ii}}{1-h_{ii}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Measures of influence: DFFITS and DFBETAS\n",
    "\n",
    "Cook's distance measure is a deletion diagnostic, it measures the influence of the $i$th observation if it is removed from the sample. There are two more useful measures of influence. The first indicates how much of the regression coefficient $\\hat{b}_j$ changes, in standard deviation units, if the $i$th observation were deleted:\n",
    "$$DFBETAS_{j, i} = \\frac{\\hat{b}_j - \\hat{b}_{j(i)}}{\\sqrt{S_{(i)}^2C_{jj}}},$$\n",
    "where $\\hat{b}_{j(i)}$ is the $j$th coefficient computed without use of the $i$th observation. A large value of DFBETAS indicates that observation $i$ has considerable influence on the $j$th regression coefficient. \n",
    "\n",
    "Define the $p \\times n$ matrix $R=(X'X)^{-1}X'$. The $n$ elements in the $j$th row of $R$ produce the leverage that the $n$ observations in the sample have on $\\hat{b}_j$. If we let $r_j'$ denote the $j$th row of $R$, then we can show that:\n",
    "$$DFBETAS_{j,i} = \\frac{r_{j,i}}{\\sqrt{r_j'r_j}}\\frac{t_i}{\\sqrt{1-h_{ii}}},$$\n",
    "where $t_i$ is the $R$-student residual. A cutoff of $2/\\sqrt{n}$ is usually applied, meaning that if $|DFBETAS_{j,i}| > 2/\\sqrt{n}$, then the observation warrants examination.\n",
    "\n",
    "We may also investigate the deletion influence of the $i$th observation on the predicted value, which leads to the second diagnostic:\n",
    "$$DFFITS_i = \\frac{\\hat{y}_i - \\hat{y}_{(i)}}{\\sqrt{S_{(i)}^2h_{ii}}}, \\quad i=1,...,n.$$\n",
    "\n",
    "It is the number of standard deviations that the fitted value $\\hat{y}_i$ changes if observation $i$ is removed. Computationally, \n",
    "\n",
    "$$DFFITS_i = \\bigg( \\frac{h_{ii}}{1-h_{ii}} \\bigg)^{0.5}t_i,$$\n",
    "where $t_i$ is R-student. The cutoff is $2\\sqrt{p/n}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 A Measure of Model Performance\n",
    "\n",
    "Diagnostics $D_i, DFBETAS_{j,i}, DFFITS_{i}$ provide insight about the effect of observations on the estimates coefficients $\\hat{b}_j$ and fitted values $\\hat{y}_i$, they do not provide any information about the overall precision of estimation. \n",
    "\n",
    "We define the generalized variance of $\\hat{b}$ as:\n",
    "$$GV(\\hat{b}) = |\\sigma^2(X'X)^{-1}|.$$\n",
    "\n",
    "To express the role of the $i$th observation on the precision of estimation, we could define:\n",
    "$$COVRATIO_i = \\frac{|(X'_{(i)}X_{(i)})^{-1}S_{(i)}^2|}{| (X'X)^{-1}MS_{Res} |}, \\quad i=1,...,n.$$\n",
    "\n",
    "It can be shows that this is:\n",
    "$$COVRATIO_i = \\frac{(S^2_{(i)})^p}{MS_{Res}^p} \\frac{1}{1-h_{ii}}.$$\n",
    "\n",
    "If $COVRATIO_i > 1$, the $i$th observation improves the precision of estimation, while if $COVRATIO_i < 1$, inclusion of the $i$th point degades precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Detecting groups of influential observations\n",
    "\n",
    "\n",
    "We have focused on single-observation deletion diagnostics for influence and leverage. There could be situations whee a group of points have high leverage or exert undue influence on the regression model. In principle, we can extend the single-observaion diagnostics to the multiple observation case. \n",
    "\n",
    "Let $i$ denote the $m \\times 1$ vector of indices specifying the points to be deleted, and define:\n",
    "$$D_i(X'X, pMS_{Res}) = \\frac{(\\hat{b}_{(i)} - \\hat{b})'X'X(\\hat{b}_{(i)} - \\hat{b})}{pMS_{Res}}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.7 Treatment of influential observations\n",
    "\n",
    "Should influential observations ever be discarded? This question is analogous to the question regarding discarding outliers. As a general rule, if there is an error in recording a measured value or if the sample point is indeed invalid or not part of the population that was intended to be sampled, then discarding the observation is appropriate. However, if analysis reveals that an influential point is a valid observa- tion, then there is no justification for its removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
