{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Simple Linear Regression\n",
    "\n",
    "## 2.1 Simple Linear Regression Model\n",
    "\n",
    "This chapter considers the simple linear regression model:\n",
    "\n",
    "$$y = b_0 + b_1x + e,$$\n",
    "where the intercept $b_0$ and the slope $b_1$ are unknown constants, and $e$ is a random error component. The errors are assumed to have mean zero and unknown variance $\\sigma^2$. Additionally, we assume that the errors are uncorrelated.\n",
    "\n",
    "For each possible value of $x$, there is a probability distribution for $y$, and the mean of the distribution is:\n",
    "$$E[y | x] = b_0 + b_1x,$$\n",
    "and the variance:\n",
    "$$Var(y | x) = Var(b_0 + b_1x + e) = \\sigma^2.$$\n",
    "\n",
    "The mean of $y$ is a linear function of $x$, but the variance does not depend on $x$. Since errors are uncorrelated, so are the responses. The parameters $b_0, b_1$ are usually called *regression coefficients*.\n",
    "\n",
    "## 2.2 Least-square estimation of the parameters\n",
    "\n",
    "The parameters $b_0, b_1$ are unknown and must be estimated using data. Suppose that we have $n$ pairs of data, $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$. The method of least squares is used to estimate parameters:\n",
    "\n",
    "$$S(b_0, b_1) = \\sum_{i=1}^{n} (y_i - b_0 - b_1x_i)^2.$$\n",
    "\n",
    "Partial derivation gives the following equations:\n",
    "$$n\\hat{b}_0 + \\hat{b}_1\\sum_{i=1}^{n}x_i = \\sum_{i=1}^{n}y_i,$$\n",
    "\n",
    "$$\\hat{b}_0\\sum_{i=1}^{n}x_i + \\hat{b}_1\\sum_{i=1}^{n}x_i^2 = \\sum_{i=1}^{n}x_iy_i.$$\n",
    "\n",
    "\n",
    "The solutions to the *least-squares normal equations* is:\n",
    "$$\\hat{b}_0 = \\overline{y} - \\hat{b}_1\\overline{x},$$\n",
    "$$\\hat{b}_1 = \\frac{\\sum_{i=1}^{n}x_iy_i - \\frac{\\sum y_i \\sum x_i}{n}}{\\sum x_i^2 - \\frac{(\\sum x_i)^2}{n}}.$$\n",
    "\n",
    "The fitted simple linear regression model is:\n",
    "$$\\hat{y} = \\hat{b}_0 + \\hat{b}_1x.$$\n",
    "\n",
    "We introduce useful notation:\n",
    "- $$S_{xx} = \\sum_{i=1}^{n}x_i^2 - \\frac{(\\sum_{i=1}^{n}x_i)^2}{n} = \\sum_{i=1}^{n}(x_i - \\overline{x})^2,$$\n",
    "- $$S_{xy} = \\sum_{i=1}^{n}x_iy_i - \\frac{(\\sum x_i)(\\sum y_i)}{n} = \\sum_{i=1}^{n} y_i(x_i - \\overline{x}). $$\n",
    "\n",
    "With this notation: $\\hat{b}_1 = S_{xy} / S_{xx}$. The difference between the observed value $y_i$ and the corresponding fitted value $\\hat{y}_i$ is a residual:\n",
    "$$e_i = y_i - \\hat{y}_i, \\quad i=1,...,n.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from linear_regression import LinearRegression\n",
    "\n",
    "x = [15.5, 23.75, 8.0, 17.0, 5.5, 19.0, 24.0, 2.5, 7.5, 11.0, 13.0, 3.75, 25.0, 9.75, 22.0, 18.0, 6.0, 12.5, 2.0, 21.5]\n",
    "y = [2158.70, 1678.15, 2316.0, 2061.3, 2207.5, 1708.3, 1784.7, 2575.0, 2357.90, 2256.70, \n",
    "     2165.20, 2399.55, 1779.80, 2336.75, 1765.30, 2053.50, 2414.40, 2200.50, 2654.20, 1753.70]\n",
    "\n",
    "model = LinearRegression(x, y)\n",
    "params = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'b0': 2627.822359001295, 'b1': -37.15359094490517}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this sample dataset, the least-squares fit for the data is:\n",
    "$$\\hat{y} = 2627.82 - 37.15x.$$\n",
    "\n",
    "\n",
    "After the model parameters have been obtained, a number of interesting questions come to mind:\n",
    "1. how well does the equation fit the data?\n",
    "2. is the model likely to be useful as a predictor?\n",
    "3. are any of the basic assumptions (constant variance and uncorrelated errors) violated?\n",
    "\n",
    "### 2.2.2 Properties of the least-squares estimators and the fitted regression model\n",
    "\n",
    "The least-squares estimators have some interesting properties:\n",
    "1. both $\\hat{b}_0, \\hat{b}_1$ are linear combinations of the observations $y_i$,\n",
    "2. they are unbiased estimators, meaning $E[b_i] = b_i, i=1, 2$,\n",
    "3. $$Var(\\hat{b}_1) = \\frac{\\sigma^2}{S_{xx}},$$\n",
    "4. $$Var(\\hat{b}_0) = \\sigma^2 \\big( \\frac{1}{n} + \\frac{\\overline{x}^2}{S_{xx}} \\big),$$\n",
    "5. Gauss-Markov property, which states that with the assumptions $E[e]=0, Var(e)=\\sigma^2$, then the least-squares estimators are unbiased and have the minimum variance when compared with all other unbiased estimators that are linear combinations of $y_i$.\n",
    "\n",
    "\n",
    "Additionally:\n",
    "1. The sum of residuals is always zero:\n",
    "$$\\sum_{i=1}^{n}(y_i - \\hat{y}_i) = \\sum_{i=1}^{n}e_i = 0,$$\n",
    "2. The least-squares regression line passes through the centroid $(\\overline{x}, \\overline{y}).$\n",
    "3. The weighted sum of residuals weighted by corresponding value of the regressor is always 0:\n",
    "$$\\sum_{i=1}^{n}x_ie_i = 0,$$\n",
    "4. The sum of the residuals weighted by the corresponding fitted value always equals zero:\n",
    "$$\\sum_{i=1}^{n}\\hat{y}_ie_i = 0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Estimation of $\\sigma^2$\n",
    "\n",
    "First, we introduce the error sum of squares:\n",
    "$$SS_{Res} = \\sum_{i=1}^{n}e_i^2 = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2,$$\n",
    "\n",
    "and the total sum of squares:\n",
    "$$SS_T = \\sum_{i=1}^{n}(y_i - \\overline{y})^2.$$\n",
    "\n",
    "They are connected through the equation:\n",
    "$$SS_{Res} = SS_T - \\hat{b}_1S_{xy}.$$\n",
    "\n",
    "It can be shown that $E[SS_{Res}] = (n-2)\\sigma^2,$ so an unbiased estimator of $\\sigma^2$ is:\n",
    "$$\\hat{\\sigma}^2 = \\frac{SS_{Res}}{n-2} = MS_{Res}.$$\n",
    "\n",
    "The quantity $MS_{Res}$ is called the *residual mean square*, and the square root of $\\hat{\\sigma}^2$ is sometimes called the *standard error of regression*. Since it is computed from the regression model residuals, we say that it is a model-dependent estimate of $\\sigma^2.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Hypothesis testing on the slope and intercept\n",
    "\n",
    "In order to test hypothesis on the slope and intercept parameters, along with constructing confidence intervals, an additional assumption is required, that model errors $e_i$ are normally distributed:\n",
    "$$e \\sim N(0, \\sigma^2).$$\n",
    "\n",
    "In chapter 4, we discuss how these assumptions can be checked through residual analysis.\n",
    "\n",
    "Suppose we wanted to test that the slope equal a particular value: $H_0: b_1 = b_{10}.$ Since errors follow the $N(0, \\sigma^2)$ distribution, the observations follow the $N(\\hat{b}_0 + \\hat{b}_1x_i, \\sigma^2).$ Now, $\\hat{b}_1$ is a linear combination of the observations, so $\\hat{b}_1$ is normally distributed with mean $b_1$ and variance $\\sigma^2/S_{xx}$. Therefore the statistic:\n",
    "\n",
    "$$Z = \\frac{\\hat{b}_1 - b_{10}}{\\sqrt{\\sigma^2 / S_{xx}}} \\sim N(0, 1),$$\n",
    "if the null hypothesis is true. In practice, $\\sigma^2$ is unknown, so this cannot be used. In previous sections, we discussed that $MS_{Res}$ is an unbiased estimator of $\\sigma^2$. It can be established that:\n",
    "$$\\frac{(n-2)MS_{Res}}{\\sigma^2} \\sim \\chi_{n-2}^2,$$\n",
    "and that $MS_{Res}$ and $\\hat{b}_1$ are independent.\n",
    "\n",
    "By the definition of the $t$-statistic:\n",
    "\n",
    "$$t_0 = \\frac{\\hat{b}_1 - b_{10}}{\\sqrt{MS_{Res}/S_{xx}}} \\sim t(n-2),$$\n",
    "\n",
    "if the null hypothesis is true. The denominator of the test statistic is often called the estimated standard error, or just standard error of the slope:\n",
    "$$SE(\\hat{b}_1) = \\sqrt{\\frac{MS_{Res}}{S_{xx}}}.$$\n",
    "\n",
    "Using this abbreviation the standard error can be written as:\n",
    "$$t_0 = \\frac{\\hat{b}_1 - b_{10}}{se(\\hat{b}_1)}.$$\n",
    "\n",
    "A similar procedure can be used to test the hypothesis about the intercept:\n",
    "$H_0: b_0 = b_{00}$, where the test statistic is:\n",
    "$$t_0 = \\frac{\\hat{b}_0 - b_{00}}{\\sqrt{MS_{Res}(1/n + \\overline{x}^2/S_{xx})}} = \\frac{\\hat{b}_0 - b_{00}}{se(\\hat{b}_0)}.$$\n",
    "\n",
    "A special case is to check $H_0: b_1=0$, which relates to the significance of the model. Failing to reject this implies that there is no linear relationship between $x, y$. This either means that $x$ is of little value in explaining the variation in $y$ and that the best estimator of $y$ is not linear. Therefore, failing to reject $H_0: b_1=0$ is equivalent to saying that there is no linear relationship between $y$ and $x$, or that the relatioship is not linear. In this special case, the test statistic comes up to:\n",
    "$$t_0 = \\frac{\\hat{b}_1}{se(\\hat{b}_1)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of the t-statistic with 18 degrees of freedom is: -12.86\n",
      "The 0.025 quantile of the t-distribution with 18 degrees of freedom is: -2.10\n",
      "The 0.975 quantile of the t-distribution with 18 degrees of freedom is: 2.10\n",
      "Reject H0: b1 = 0\n"
     ]
    }
   ],
   "source": [
    "model.regression_significance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3. Analysis of variance\n",
    "\n",
    "We may also use an *analysis of variance* approach to test significance of regression. The analysis of variance is based on a partitioning of total variability in the response variable $y$. To obtain this partitioning, begin with the identity\n",
    "\n",
    "$$y_i - \\overline{y} = (\\hat{y}_i - \\overline{y}) + (y_i - \\hat{y}_i).$$\n",
    "\n",
    "From this, we can derive the identity:\n",
    "\n",
    "$$\\sum_{i=1}^{n}(y_i - \\overline{y})^2 = \\sum_{i=1}^{n}(\\hat{y}_i - \\overline{y})^2 + \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2.$$\n",
    "This can be written as:\n",
    "$$SS_T = SS_R + SS_{Res},$$\n",
    "where.\n",
    "- $SS_T$ is the total variability in the observations ($df_T = n-1$),\n",
    "- $SS_R$ is the amount of variability is the observations accounted for by the regression line($df_R=1$),\n",
    "- $SS_{Res}$ residual variability left unexplained ($df_{Res}=n-2$).\n",
    "\n",
    "We can use the usual analysis of variance $F$ test to test the hypothesis that $H_0: b_1=0$. It can be shown that:\n",
    "- $$SS_{Res} = \\frac{(n-2)MS_{Res}}{\\sigma^2} \\sim \\chi^2_{n-2}$$\n",
    "- if the null hypothesis is true, then \n",
    "$$\\frac{SS_R}{\\sigma^2} \\sim \\chi^2_{1},$$\n",
    "- $SS_{Res}, SS_R$ are independent.\n",
    "\n",
    "By the definition of the $F$ statistic:\n",
    "$$F_0 = \\frac{SS_R/df_R}{SS_{Res}/df_{Res}} = \\frac{MS_R}{MS_{Res}} \\sim F(1, n-2).$$\n",
    "\n",
    "It can be shown that $E[MS_{Res}] = \\sigma^2, E[MS_R] = \\sigma^2 + b_1^2S_{xx}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Interval estimation in simple linear regression\n",
    "\n",
    "### 2.4.1 Confidence intervals on $b_0, b_1, \\sigma^2$\n",
    "\n",
    "In addition to point estimated, we can also obtain confidence interval estimates of these parameters. The width of these confidence intervals is a measure of the overall quality of the regression line. If the errors are normally and independently distributed, then the sampling distribution of both\n",
    "$$\\frac{\\hat{b}_1 - b_1}{SE(\\hat{b}_1)}, \\frac{\\hat{b}_0 - b_0}{SE(\\hat{b}_0)} \\sim t(n-2).$$\n",
    "\n",
    "Therefore, the $100(1-\\alpha)$ percent CI on the slope $b_1$ is given by:\n",
    "$$\\hat{b}_1 - t_{a/2, n-2}SE(\\hat{b}_1) \\leq \\hat{b}_1 \\leq \\hat{b}_1 + t_{a/2, n-2}SE(\\hat{b}_1),$$\n",
    "$$\\hat{b}_0 - t_{a/2, n-2}SE(\\hat{b}_0) \\leq \\hat{b}_0 \\leq \\hat{b}_0 + t_{a/2, n-2}SE(\\hat{b}_0).$$\n",
    "\n",
    "If the errors are normally and independently distributed, then \n",
    "$$\\frac{(n-2)MS_{Res}}{\\sigma^2} \\sim \\chi^2_{n-2},$$\n",
    "so the $100(1-\\alpha)$ percent CI on $\\sigma^2$ is:\n",
    "\n",
    "$$\\frac{(n-2)MS_{Res}}{\\chi^2_{\\alpha/2, n-2}} \\leq \\sigma^2 \\leq \\frac{(n-2)MS_{Res}}{\\chi_{1-\\alpha/2, n-2}^2}.$$\n",
    "\n",
    "In the above formulas:\n",
    "- $$SE(\\hat{b}_0) = \\sqrt{MS_{Res}(1/n + \\overline{x}^2/S_{xx})}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Interval estimation of the mean response\n",
    "\n",
    "A major use of a regression model is to estimate the mean response $E(y)$ for a particular value of the regressor variable $x$. Ley $x_0$ be the level of the regressor variable for which we wish to estimate the mean response, say $E[y|x_0]$. An unbiased estimator of $E[y|x_0]$ is found from the fitted model as:\n",
    "$$\\hat{E[t|x_0]} = \\hat{\\mu}_{y|x_0} = \\hat{b}_0 + \\hat{b}_1x_0.$$\n",
    "\n",
    "To obtain a $100(1-\\alpha)$ percent CI on $E[y|x_0]$ first note that $\\hat{\\mu}_{y|x_0}$ is a normally distributed random variable because it is a linear combination of the observations $y_i$. The variance is:\n",
    "$$Var(\\hat{\\mu}_{y—x_0}) = \\sigma^2 \\bigg[ \\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}} \\bigg].$$\n",
    "\n",
    "This is a consequence of the fact that $Cov(\\overline{y}, \\hat{b}_1)=0$, and thus the sampling method:\n",
    "$$\\frac{\\hat{\\mu}_{y—x_0} - E[y|x_0]}{\\sqrt{MS_{Res}(1/n + (x_0 - \\overline{x})^2/S_{xx})}} \\sim t(n-2).$$\n",
    "\n",
    "Finally, the $100(1-\\alpha)$ CI on the mean response at the point $x=x_0$ is:\n",
    "\n",
    "$$\\hat{\\mu}_{y|x_0} - t_{\\alpha/2, n-2} {\\sqrt{MS_{Res} \\bigg( \\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}  \\bigg)}} \\leq E[y|x_0] \\leq \\hat{\\mu}_{y|x_0} + t_{\\alpha/2, n-2} {\\sqrt{MS_{Res} \\bigg( \\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}  \\bigg)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Prediction of new observations\n",
    "\n",
    "An important application of the regression model is prediction of new observations $y$ corresponding to a specified level of the regressor variable $x$. If $x_0$ is the value of the regressor variable of interest, then:\n",
    "$$\\hat{y}_0 = \\hat{b}_0 + \\hat{b}_1x_0$$\n",
    "is the point estimate of the new value of the response $y_0$. We develop a prediction interval for the future observations $y_0$.\n",
    "\n",
    "Note that the random variable \n",
    "$$\\psi = y_0 - \\hat{y}_0$$\n",
    "follows a normal distribution with mean zero and variance:\n",
    "\n",
    "$$Var(\\psi) = Var(y_0 - \\hat{y}_0) = \\sigma^2 \\bigg[ 1+\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}} \\bigg],$$\n",
    "because the future observation $y_0$ is independent of $\\hat{y}_i.$\n",
    "\n",
    "Thus, the $100(1-\\alpha)$ percent prediction interval on a future observation at $x_0$ is:\n",
    "\n",
    "$$\\hat{y}_0 - t_{\\alpha/2, n-2}\\sqrt{MS_{Res}\\big( 1+\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}} \\big)} \\leq y_0 \\leq \\hat{y}_0 + t_{\\alpha/2, n-2}\\sqrt{MS_{Res}\\big( 1+\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}} \\big)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Coefficient of determination\n",
    "\n",
    "The quantity:\n",
    "$$R^2 = \\frac{SS_R}{SS_T} = 1- \\frac{SS_{Res}}{SS_T} \\in [0, 1]$$\n",
    "is called the *coefficient of determination*. Since $SS_T$ is a measure of the variability in $y$ without considering the effect of the regressor variable $x$ and $SS_{Res}$ is a measure of the variability in $y$ remaining after $x$ has been considered, $R^2$ is often called the proportion of variation explained by the regressor $x$.\n",
    "\n",
    "Although $R^2$ cannot decrease if we add a regressor variable to the model, this\n",
    "does not necessarily mean the new model is superior to the old one. Unless the error sum of squares in the new model is reduced by an amount equal to the original error mean square, the new model will have a larger error mean square than the old one because of the loss of one degree of freedom for error. Thus, the new model will actually be worse than the old one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10. Regression Through the Origin\n",
    "\n",
    "The no-intercept model is: $y = b_1x + e.$ Given $n$ observations $(x_i, y_i), i=1,...,n$, the least-squares function is:\n",
    "\n",
    "$$S(b_1) = \\sum_{i=1}^{n} (y_i - b_1x_i)^2.$$\n",
    "\n",
    "In this special case:\n",
    "\n",
    "- the least-square estimate of the slope is:\n",
    "$$\\hat{b}_1 = \\frac{\\sum_{i=1}^{n}x_iy_i}{\\sum_{i=1}^{n}x_i^2},$$\n",
    "\n",
    "- the estimator of $\\sigma^2$ (with $n-1$ degrees of freedom):\n",
    "\n",
    "$$\\hat{\\sigma}^2 = MS_{Res} = \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n-1},$$\n",
    "\n",
    "- making the normality assumption on the errors, we may test the hypotheses and construct confidence and prediction intervals for the no-intercept model. The $100(1-\\alpha)$ percent CI for $b_1$ is:\n",
    "\n",
    "$$\\hat{b}_1 - t_{\\alpha/2, n-1} \\sqrt{\\frac{MS_{Res}}{\\sum x_i^2}} \\leq b_1 \\leq \\hat{b}_1 + t_{\\alpha/2, n-1} \\sqrt{\\frac{MS_{Res}}{\\sum x_i^2}},$$\n",
    "\n",
    "- a $100(1-\\alpha)$ percent CI on $E[y|x_0]$, the mean response at $x=x_0$ is:\n",
    "\n",
    "$$\\hat{\\mu}_{y|x_0} - t_{\\alpha/2, n-1} \\sqrt{\\frac{x_0^2MS_{Res}}{\\sum x_i^2}} \\leq E[y|x_0] \\leq\n",
    "\\hat{\\mu}_{y|x_0} + t_{\\alpha/2, n-1} \\sqrt{\\frac{x_0^2MS_{Res}}{\\sum x_i^2}}, $$\n",
    "\n",
    "- the $100(1-\\alpha)$ percent prediction interval on a future observation at $x=x_0$ is:\n",
    "$$\\hat{y}_0 - t_{\\alpha/2, n-1} \\sqrt{MS_{Res} \\big( 1+\\frac{x_0^2}{\\sum x_i^2} \\big)} \\leq y_0 \\leq \n",
    "\\hat{y}_0 + t_{\\alpha/2, n-1} \\sqrt{MS_{Res} \\big( 1+\\frac{x_0^2}{\\sum x_i^2} \\big)}.\n",
    "$$\n",
    "\n",
    "In this special case, the no-intercept analogue for $R^2$ is:\n",
    "$$R_0^2 = \\frac{\\sum_{i=1}^{n} \\hat{y}_i^2 }{ \\sum_{i=1}^{n} y_i^2 }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEb1: 2.89\n",
      "t-value: -2.10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-43.22337857013983, -31.083803319670505)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.confidence_interval_estimate(\"b1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
