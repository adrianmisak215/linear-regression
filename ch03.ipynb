{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Multiple Linear Regression\n",
    "\n",
    "In general, the response variable $y$ may be related to $k$ regressor or predictor variables. The model:\n",
    "$$y = b_0 + b_1x_1 + b_2x_2 + b_3x_3 + ... + b_kx_k + e,$$\n",
    "is called a multiple linear regression model with $k$ regressors. The parameter $b_j$ represents the expected change in th response $y$ per unit change in $x_j$, when all of the remaining regressors remain constant.\n",
    "\n",
    "## 3.2 Estimation of model parameters\n",
    "\n",
    "The method of least squares can be used to estimate the regression coefficients. We assume that the error term follows the distribution $N(0, \\sigma^2)$, and that the errors are uncorrelated. Furthermore, we assume that the regressor variables are measured without error.\n",
    "\n",
    "The following notation is introduced:\n",
    "\n",
    "$$y = \\begin{bmatrix}\n",
    "y_1 \\\\\n",
    "y_2 \\\\\n",
    "... \\\\\n",
    "y_n\n",
    "\\end{bmatrix}, \\quad \n",
    "b = \\begin{bmatrix}\n",
    "b_0 \\\\\n",
    "b_1 \\\\\n",
    "... \\\\\n",
    "b_n\n",
    "\\end{bmatrix}, \\quad \n",
    "e = \\begin{bmatrix}\n",
    "e_1 \\\\\n",
    "e_2 \\\\\n",
    "... \\\\\n",
    "e_n\n",
    "\\end{bmatrix}, \\quad\n",
    "X = \\begin{bmatrix}\n",
    "1 && x_{11} && x_{12} && ... && x_{1k} \\\\\n",
    "1 && x_{21} && x_{22} && ... && x_{2k} \\\\\n",
    "... && ... && ... && ... && ... \\\\\n",
    "1 && x_{n1} && x_{n2} && ... && x_{nk} \\\\\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "We wish to find the vector of least-squares estimators, $\\hat{b}$ that minimizes:\n",
    "$$S(b) = ||y-Xb||_2^2.$$\n",
    "\n",
    "\n",
    "The least-squares estimator $b$ is:\n",
    "$$\\hat{b} = (X'X)^{-1}X'y,$$\n",
    "provided that the inverse matrix exists. It will always exist if the regressors are linearly independent. \n",
    "\n",
    "The vector of fitted values $\\hat{y}_i$ corresponding to the observed values $y_i$ is:\n",
    "$$\\hat{y} = X(X'X)^{-1}X'y = Hy.$$\n",
    "\n",
    "The $n \\times n$ matrix $H=X(X'X)^{-1}X'$ is usually called the hat matrix. The difference between the observed value $y_i$ and the corresponding fitted value $\\hat{y}_i$ is the residual $e = y - \\hat{y} = (I-H)y.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.34123115 1.61590721 0.01438483]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import linear_regression as lr\n",
    "\n",
    "delivery_times = [16.68, 11.50, 12.03, 14.88, 13.75, 18.11, 8.0, 17.83, 79.24, 21.5, 40.33, 21.0, 13.50, 19.75, 24.0, 29.0, 15.35, 19.0, 9.5, 35.1, 17.9, 52.32, 18.75, 19.83, 10.75]\n",
    "number_cases = [7, 3, 3, 4, 6, 7, 2, 7, 30, 5, 16, 10, 4, 6, 9, 10, 6, 7, 3, 17, 10, 26, 9, 8, 4]\n",
    "distances = [560, 220, 340, 80, 150, 330, 110, 210, 1460, 605, 688, 215, 255, 462, 448, 776, 200, 132, 36, 770, 140, 810, 450, 635, 150]\n",
    "\n",
    "X = np.array([np.ones(len(delivery_times)), number_cases, distances]).T\n",
    "y = np.array(delivery_times)\n",
    "\n",
    "model = lr.MultipleLinearRegression(X, y)\n",
    "print(model._parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Properties of the Least-Squares Estimators\n",
    "\n",
    "It can be proven that $E[\\hat{b}] = b$, and since $E[e] = 0$ the $\\hat{b}$ is an unbiased estimator of $b$ if the model is correct. The variance of the estimates is:\n",
    "$$Var(\\hat{b}) = \\sigma^2 (X'X)^{-1}.$$\n",
    "\n",
    "To simplify, we introduce the notation $C = (X'X)^{-1}$, and so:\n",
    "- the variance of $\\hat{b}_j$ is $\\sigma^2 C_{jj}$,\n",
    "- covariance between $\\hat{b}_i$ and $\\hat{b}_j$ is $\\sigma^2 C_{ij}.$\n",
    "\n",
    "The Gauss Markov theorem also states that the least-squares estimator is the best unbiased estimator of $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.4 Estimation of $\\sigma^2$\n",
    "\n",
    "As in the simple linear regression, we may develop the estimator of $\\sigma^2$ from the residual sum of squares:\n",
    "$$SS_{Res} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} e_i^2 = (e, e).$$\n",
    "\n",
    "By substituting $e=y-X\\hat{b}$, the last equation becomes:\n",
    "$$SS_{Res} = (y, y) - (\\hat{b}'X', y).$$\n",
    "\n",
    "The residual sum of squares has $n-p$ degrees of freedom associated with it since $p$ parameters are estimated in the regression model. The residual mean square is:\n",
    "\n",
    "$$MS_{Res} = \\frac{SS_{Res}}{n-p}.$$\n",
    "\n",
    "Furthermore, it can be shown that the expected value of $MS_{Res}$ is $\\sigma^2$, so this is an unbiased estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233.7316774205392\n"
     ]
    }
   ],
   "source": [
    "print(model.SSRes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.62416715547967\n"
     ]
    }
   ],
   "source": [
    "print(model.MSRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Hypothesis testing in multiple linear regression\n",
    "\n",
    "Once the parameters have been estimated, two questions need to be addressed:\n",
    "1. what is the overall adequacy of the model?\n",
    "2. which specific regressors seem important?\n",
    "\n",
    "The formal tests require that the random errors are independent and follow the $N(0, \\sigma^2)$ distribution. \n",
    "\n",
    "\n",
    "### 3.3.1 Test for significance of regression\n",
    "\n",
    "\n",
    "The test for significance of regression is a test to determine if there is a linear relationship between the response $y$ and any of the regressor variables $x_1, x_2, . . . , x_k$. This procedure is often thought of as an overall or global test of model adequacy. The appropriate hypotheses are:\n",
    "- $H_0: b_1=b_2=...=b_k=0$,\n",
    "- $H_1: b_j \\neq 0$, for at least one $j$.\n",
    "\n",
    "Rejecting the null hypothesis means that at least one regressor contributes significantly to the model. The test procedure is the generalization of the analysis of variance from the simple linear regression:\n",
    "$$SS_T = SS_R + SS_{Res}.$$\n",
    "\n",
    "The central result is that if the null hypothesis is true, then:\n",
    "- $$\\frac{SS_R}{\\sigma^2} \\sim \\chi^2_{k},$$\n",
    "- $$\\frac{SS_{Res}}{\\sigma^2} \\sim \\chi^2{n-k-1},$$\n",
    "- $SS_R, SS_{Res}$ are independent.\n",
    "\n",
    "Then,\n",
    "$$F_0 = \\frac{SS_R / k}{SS_{Res} / (n-k-1)} = \\frac{MS_R}{MS_{Res}} \\sim F(k, n-k-1).$$\n",
    "\n",
    "Furthermore, it can be shown that:\n",
    "- $E[MS_{Res}] = \\sigma^2$,\n",
    "- $E[MS_R] = \\sigma^2 + \\frac{\\beta'X_c'X_c\\beta}{k\\sigma^2},$\n",
    "\n",
    "where $\\beta = (b_1, ..., b_n)', $ and\n",
    "$$X_c = \\begin{bmatrix}\n",
    "x_{11} - \\overline{x}_1 && x_{12} - \\overline{x}_2 && ... && x_{1k} - \\overline{x}_k \\\\\n",
    "x_{21} - \\overline{x}_1 && x_{22} - \\overline{x}_2 && ... && x_{2k} - \\overline{x}_k \\\\\n",
    "... && ... && ... && ... \\\\\n",
    "x_{n1} - \\overline{x}_1 && x_{n2} - \\overline{x}_2 && ... && x_{nk} - \\overline{x}_k \\\\\n",
    "\\end{bmatrix}$$\n",
    "is the centered model matrix.\n",
    "\n",
    "If at least one $b_j \\neq 0$, then $F_0$ follows a noncentral $F$ distribution with $k$ and $n-k-1$ degrees of freedom and a noncentrality parameter of:\n",
    "$$\\lambda = \\frac{\\beta'X_c'X_c \\beta}{\\sigma^2}.$$\n",
    "\n",
    "The individual values from the analysis of variance are:\n",
    "- regression sum of squares: $$SS_R = (X\\hat{b}, y)- \\frac{(\\sum y_i)^2}{n},$$\n",
    "\n",
    "- residual sum of squares: $$SS_{Res} = (y, y) - (X\\hat{b}, y),$$\n",
    "\n",
    "- total sum of squares: $$SS_T = (y, y) - \\frac{(\\sum y_i)^2}{n}.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis of Variance:\n",
      "  - SST: 5784.5426\n",
      "  - SSR: 5550.8109\n",
      "  - SSRes: 233.7317\n"
     ]
    }
   ],
   "source": [
    "print(\"Analysis of Variance:\")\n",
    "print(\"  - SST: {:.4f}\".format(model.SST))\n",
    "print(\"  - SSR: {:.4f}\".format(model.SSR))\n",
    "print(\"  - SSRes: {:.4f}\".format(model.SSRes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some regression model builders prefer to use the adjusted $R^2$ statistic:\n",
    "$$R^2_{adj} = 1 - \\frac{SS_{Res}/(n-p)}{SS_T / (n-1)}.$$\n",
    "\n",
    "Since $SS_{Res}/(n − p)$ is the residual mean square and $SS_T/(n − 1)$ is constant regardless of how many variables are in the model, $R^2_{adj}$ will only increase on adding a variable to the model if the addition of the variable reduces the residual mean square. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Tests on Individual Regression Coefficients and Subsets of Coefficients\n",
    "\n",
    "Once we have determined that at least one of the regressors is important, a logical question becomes which one(s). Adding a variable to a regression model always causes the sum of squares for regression to increase and the residual sum of squares to decrease. We must decide whether the increase in the regression sum of squares is sufficient to warrant using the additional regressor in the model. The addition of a regressor also increases the variance of the fitted value $\\hat{y}$ , so we must be careful to include only regressors that are of real value in explaining the response. Furthermore, adding an unimportant regressor may increase the residual mean square, which may decrease the usefulness of the model.\n",
    "\n",
    "The hypotheses for testing the significance of any individual regression coefficient are:\n",
    "- $H_0: b_j = 0$,\n",
    "- $H_1: b_j \\neq 0$.\n",
    "\n",
    "If the hypothesis is not rejected, then the regressor can be deleted from the model. The test statistic is:\n",
    "$$t_0 = \\frac{\\hat{b}_j}{\\sqrt{\\hat{\\sigma}^2C_{jj}}},$$\n",
    "\n",
    "where $C_{jj}$ is the diagonal element of $(X'X)^{-1}$ corresponding to $\\hat{b}_j$. Note that this is really a partial or marginal test because the regression coefficient $\\hat{b}_j$ depends on all of the other regressor variables $x_i, i\\neq j$ that are in the model. Thus, this is a test of the contribution of $x_j$ given the other regressors in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also directly determine the contribution to the regression sum of squares of a regressor, for example, $x_j$, given that other regressors $x_i, i\\neq j$ are included in the model by using the extra-sum-of-squares method. This procedure can also be used to investigate the contribution of a subset of the regressor variables to the model.\n",
    "\n",
    "We would like to determine if some subset of $r < k$ regressors contributes significantly to the regression model. Let the vector of regression coefficients be partitioned as follows:\n",
    "$$\\beta = \\begin{bmatrix}\n",
    "\\beta_1 \\\\\n",
    "\\beta_2\n",
    "\\end{bmatrix},$$\n",
    "\n",
    "where $\\beta_1$ is $(p-r) \\times 1$, and $\\beta_2$ is $r \\times 1$. We wish to test the hypothesis:\n",
    "- $H_0: \\beta_2 = 0$,\n",
    "- $H_1: \\beta_2 \\neq 0.$\n",
    "\n",
    "The model can be written as:\n",
    "$$y = Xb + e = X_1 \\beta_1 + X_2 \\beta_2 + e,$$\n",
    "\n",
    "where the $n \\times (p-r)$ matrix $X_1$ represents the columns of $X$ associated with $\\beta_1$ and the $n\\times r$ matrix $X_2$ represents the columns of $X$ associated with $\\beta_2$. This is called the full model.\n",
    "\n",
    "To find the contribution of the terms in $\\beta_2$ to the regression, fit the model assuming that the null hypothesis $H_0: \\beta_2 = 0$ is true. This reduced model is:\n",
    "$$y = X_1 \\beta_1 + e.$$\n",
    "\n",
    "The least-squares estimator of $\\beta_1$ in the reduced model is: $\\hat{\\beta}_1 = (X_1'X_1)^{-1}X_1'y$, and the regression sum of squares is:\n",
    "$$SS_R(\\beta_1) = (X_1\\hat{\\beta}_1, y),$$\n",
    "with $p-r$ degrees of freedom. The regression sum of squares due to $\\beta_2$ given that $\\beta_1$ is already in the model is:\n",
    "$$SS_R(\\beta_2 | \\beta_1) = SS_R(\\beta) - SS_R(\\beta_1),$$\n",
    "with $r$ degrees of freedom. \n",
    "\n",
    "It can be shown that $SS_R(\\beta_2 | \\beta_1)$ is independent of $MS_{Res}$, so the null hypothesis that $\\beta_2 = 0$ can be tested with the statistic:\n",
    "$$F_0 = \\frac{SS_R(\\beta_2 | \\beta_1) / r}{MS_{Res}}.$$\n",
    "\n",
    "If $\\beta_2 \\neq 0$, then $F_0$ follows a noncentral $F$ distribution with a noncentrality parameter of \n",
    "$$\\lambda = \\frac{1}{\\sigma^2} \\beta_2'X_2'[I - X_1(X_1'X_1)^{-1}X_1']X_2 \\beta_2.$$\n",
    "\n",
    "If $F_0 > F_{\\alpha, r, n-p}$, we reject $H_0$ concluding that at least one of the parameters in $\\beta_2$ is not zero, and consequently at least one regressor from $X_2$ contributes significantly to the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the model:\n",
    "$$y = b_0 + b_1x_1 + b_2x_2 + b_3x_3 + e.$$\n",
    "\n",
    "The sums of squares $SS_R(b_1 | b_0, b_2, b_3), SS_R(b_2 | b_0, b_1, b_3), SS_R(b_3 | b_0, b_1, b_2)$ are single-degree-of-freedom sums of squares that measure the contribution of each regressor $x_j$ to the model given that all other regressors were already in the model. That is, we are assessing the value of adding $x_j$ to a model that did not include this regressor.\n",
    "\n",
    "In general, we could find \n",
    "$$SS_R(b_j | b_0, b_1, ..., b_{j-1}, b_{j+1}, ..., b_k),$$\n",
    "which is the increase in the regression sum of squares due to adding $x_j$ to a model that already contains other regressors. Some find it helpful to think of this as measuring the contribution of $x_j$ as if it were the last variable added to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3. Special Case of Orthogonal Columns in $X$\n",
    "\n",
    "Consider the model\n",
    "$$ y = Xb + e $$\n",
    "$$ y = X_1\\beta_1 + X_2\\beta_2 + e$$\n",
    "\n",
    "The extra-sum-of-squares method allows us to measure the effect of the regressors in $X_2$ conditional on those in $X_1$ by computing $SS_R(\\beta_2 | \\beta_1).$ Additionally, if the columns in $X_1$ are orthogonal to those in $X_2$, we can determine a sum of squares due to $\\beta_2$ that is free of any dependence on the regressors in $X_1$.\n",
    "\n",
    "If the columns of $X_1$ are orthogonal to columns in $X_2$, then $X_1'X_2 = 0$, and $X_2'X_1 = 0$, and the solutions to the normal equations are given with:\n",
    "\n",
    "$$ \\hat{\\beta}_1 = (X_1'X_1)^{-1}X_1'y, \\quad \\hat{\\beta}_2 = (X_2'X_2)^{-1}X_2'y. $$\n",
    "\n",
    "The least squares estimator for parameters $\\beta_1$ does not depend if the parameters from $X_2$ are in the model or not.\n",
    "\n",
    "The regression sum of squares for the full model is:\n",
    "\n",
    "$$SS_R(\\beta) = y'X_1(X_1'X_1)^{-1}X_1'y + y'X_2(X_2'X_2)^{-1}X_2'y$$\n",
    "\n",
    "$$SS_R(\\beta) = SS_R(\\beta_1) + SS_R(\\beta_2).$$\n",
    "\n",
    "Finally, $SS_R(\\beta_1 | \\beta_2) = SS_R(\\beta) - SS_R(\\beta_2) = SS_R(\\beta_1).$\n",
    "\n",
    "Consequently, $SS_R(\\beta_1)$ measures the contribution of the regressors in $X_1$ to the model unconditionally, and $SS_R(\\beta_2)$ measures the contribution of the regressors in $X_2$ to the model unconditionally. Because we can unambiguously determine the effect of each regressor when the regressors are orthogonal, data collection experi- ments are often designed to have orthogonal variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.4 Testing the General Linear Hypothesis\n",
    "\n",
    "Suppose that the null hypothesis of interest can be expressed as $H_0: T\\beta = 0$, where $T$ is an $m \\times p$ matrix of constants, such that only $r$ of the $m$ equations in $T\\beta = 0$ are independent.\n",
    "\n",
    "The full model is $y=X\\beta + e$, with $\\hat{\\beta} = (X'X)^{-1}X'y$, and the residual sum of squares for the full model is:\n",
    "\n",
    "$$SS_{Res}(FM) = (y-X\\hat{\\beta}, y),$$\n",
    "with $n-p$ degrees of freedom. To obtain the reduced model, the $r$ independent equations in $T\\beta=0$ are used to solve for $r$ of the regression coefficients in the full model in terms of the remaining $p-r$ regression coefficients, and this leads to the reduced model:\n",
    "\n",
    "$$y = Z\\gamma + e.$$\n",
    "\n",
    "The estimate for $\\gamma$ is:\n",
    "$$\\hat{\\gamma} = (Z'Z)^{-1}Z'y,$$\n",
    "\n",
    "and the residual sum of squares is:\n",
    "\n",
    "$$SS_{Res}(RM) = (y-Z\\hat{\\gamma}, y),$$\n",
    "\n",
    "with $n-p+r$ degrees of freedom. Since the reduced model contains fewer parameters than the full model, $SS_{Res}(RM) \\geq SS_{Res}(FM).$ To test the hypothesis, we use the difference in residual sums of squares:\n",
    "$$SS_H = SS_{Res}(RM) - SS_{Res}(FM),$$\n",
    "\n",
    "with $n-p+r-(n-p)=r$ degrees of freedom. Here, $SS_H$ is called the sum of squares due to the hypothesis $H_0: T\\beta=0.$\n",
    "\n",
    "The test statistic for the hypothesis is:\n",
    "$$F_0 = \\frac{SS_H/r}{SS_{Res}(FM)/(n-p)} \\sim F(r, n-p).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Confidence intervals in multiple regression\n",
    "\n",
    "To construct confidence interval estimates for the regression coefficients $b_j$, we will continue to assume that the errors $e_i$ are normally and independently distributed with mean zero and variance $\\sigma^2$. As a result, observations $y_i$ are normally distributed with mean $b_0 + b_1x_{i1} + b_2x_{i2} + ... + b_kx_{ik}$, and variance $\\sigma^2$.\n",
    "\n",
    "A $100(1-\\alpha)$ percent confidence interval for the regression coefficient $b_j$, $j=0,1,...,k$ as:\n",
    "\n",
    "$$\\hat{b}_j - t_{\\alpha/2, n-2} \\sqrt{\\hat{\\sigma}^2C_{jj}} \\leq b_j \\leq \\hat{b}_j + t_{\\alpha/2, n-2} \\sqrt{\\hat{\\sigma}^2C_{jj}}.$$\n",
    "\n",
    "The expression $SE(\\hat{b}_j) = \\sqrt{\\hat{\\sigma}^2C_{jj}}$ is the standard deviation of the regression coefficient $\\hat{b}_j.$\n",
    "\n",
    "### 3.4.2 CI estimation of the mean response\n",
    "\n",
    "We may construct a CI on the mean response at a particular point, such as $x_0 = (1, x_{01}, ..., x_{0k})^{T}$. The fitted value at this point is:\n",
    "\n",
    "$$\\hat{y}_0 = (x_0, \\hat{b}).$$\n",
    "\n",
    "This is an unbiased estimator of $E[y | x_0],$ and the variance of $\\hat{y}_0$ is:\n",
    "\n",
    "$$Var(\\hat{y}_0) = \\sigma^2 x_0'(X'X)^{-1}x_0.$$\n",
    "\n",
    "Therefore, a $100(1-\\alpha)$ percent CI on the mean response at the point $x_0$ is:\n",
    "\n",
    "$$\\hat{y}_0 - t_{\\alpha/2, n-p}\\sqrt{\\hat{\\sigma}^2x_0'(X'X)^{-1}x_0} \\leq E[y | x_0] \\leq \n",
    "\\hat{y}_0 + t_{\\alpha/2, n-p}\\sqrt{\\hat{\\sigma}^2x_0'(X'X)^{-1}x_0}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4.3 Simultaneous Confidence Intervals on Regression Coefficients\n",
    "\n",
    "Some problems require that several confidence or prediction intervals be constructed using the same sample data. In these cases, the analyst is usually interested in specifying a confidence coefficient that applies simultaneously to the entire set of interval estimates. A set of confidence or prediction intervals that are all true simultaneously with probability $1-\\alpha$ are called simultaneous or joint confidence or joint prediction intervals.\n",
    "\n",
    "The main result is that:\n",
    "\n",
    "$$\\frac{(\\hat{b} - b)'X'X(\\hat{b} - b)}{pMS_{Res}} \\sim F_{p, n-p}.$$\n",
    "\n",
    "\n",
    "Consequently, a $100(1-\\alpha)$ percent joint confidence region for all of the parameters in $\\beta$ is:\n",
    "\n",
    "$$\\frac{(\\hat{b} - b)'X'X(\\hat{b} - b)}{pMS_{Res}} \\leq F_{\\alpha, p, n-p}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Prediction of new observations\n",
    "\n",
    "A point estimate of the future observation $y_0$ at the point $x_0 = (x_{01}, x_{02}, ..., x_{0k})$ is:\n",
    "$$\\hat{y}_0 = (\\hat{b}, x_0).$$\n",
    "\n",
    "A $100(1-\\alpha)$ percent prediction interval for the future observation is:\n",
    "\n",
    "$$\\hat{y}_0 - t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2(1+x_0'(X'X)^{-1}x_0)} \\leq y_0 \\leq\n",
    "\\hat{y}_0 + t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2(1+x_0'(X'X)^{-1}x_0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Hidden extrapolation in multiple regression\n",
    "\n",
    "In predicting new responses and in estimating the mean response at a given point $x_{01}, x_{02}, . . . , x_{0k}$ one must be careful about extrapolating beyond the region containing the original observations. In multiple regression it is easy to inadvertently extrapolate, since the levels of the regressors jointly define the region containing the data.\n",
    "\n",
    "As an example, consider Figure 3.10, which illustrates the region containing the original data for a two-regressor model. Note that the point $(x_{01}, x_{02})$ lies within the ranges of both regressors $x_1$ and $x_2$ but outside the region of the original data. Thus, either predicting the value of a new observation or estimating the mean response at this point is an extrapolation of the original regression model.\n",
    "\n",
    "\n",
    "Since simply comparing the levels of the x’s for a new data point with the ranges of the original x’s will not always detect a hidden extrapolation, it would be helpful to have a formal procedure to do so. We will define the smallest convex set containing all of the original n data points $(x_{i1}, x_{i2}, . . . , x_{ik})$, $i = 1, 2, . . . , n$, as the regressor variable hull (RVH). If a point $x_{01}, x_{02}, . . . , x_{0k}$ lies inside or on the boundary of the RVH, then prediction or estimation involves interpolation, while if this point lies outside the RVH, extrapolation is required.\n",
    "\n",
    "The diagonal elements $h_{ii}$ of the hat matrix $H=X(X'X)^{-1}X'$ are useful in detecting hidden extrapolation. The set of points $x$ that satisfy\n",
    "$$x'(X'X)^{-1}x \\leq h_{max}$$\n",
    "\n",
    "is an ellipsoid enclosing all points inside the RVH. Thus if we are interested in prediction or estimation at the point $x_0'$, the location of that point relative to the RVH is reflected by:\n",
    "$$h_{00} = x_0'(X'X)^{-1}x_0.$$\n",
    "\n",
    "Points for which $h_{00} > h_{max}$ are outside the ellipsoid enclosing the RVH and are extrapolation points. However, if $h_{00} < h_{max}$, then the point is inside the ellipsoid and possibly inside the RVH and would be considered an interpolation point because it is close to the cloud of points used to fit the model. Generally the smaller the value of $h_{00}$, the closer the point $x_0$ lies to the centroid of the x space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Standardized regression coefficients\n",
    "\n",
    "It is usually difficult to directly compare regression coefficients because the maguitude of $\\hat{b}_j$ reflects the units of measurement of the regressor $x_j$. For example, suppose that the regression model is:\n",
    "\n",
    "$$\\hat{y} = 5 + x_1 + 1000x_2,$$\n",
    "\n",
    "and $y$ is measured in liters, $x_1$ is measured in milliliters, and $x_2$ is measured in liters. Note that although $\\hat{b}_2$ is considerably larger than $\\hat{b}_1$, the effect of both regressors on $\\hat{y}$ is identical, since a 1-liter change in either $x_1$ or $x_2$ when the other variable is held constant produces the same change in $\\hat{y}$. \n",
    "\n",
    "Generally the units of the regression coefficient $b_j$ are units of $y$/units of $x_j$. For this reason, it is sometimes helpful to work with scaled regressor and response variables that produce dimensionless regression coefficients. These dimensionless coefficients are usually called standardized regression coefficients. We now show how they are computed, using two popular scaling techniques.\n",
    "\n",
    "**Unit Normal Scaling**\n",
    "\n",
    "Both the regressors and the response variable get scaled:\n",
    "$$z_{ij} = \\frac{x_{ij} - \\overline{x}_j}{s_j}, \\quad i=1, ..., n, \\quad j=1, ..., k.$$\n",
    "\n",
    "$$w_i = \\frac{y_i - \\overline{y}}{s_y}, i=1,..., n,$$\n",
    "\n",
    "where \n",
    "\n",
    "$$s_j^2 = \\frac{\\sum_{i=1}^{n}(x_{ij} - \\overline{x}_j)^2}{n-1} \\quad s_y^2 = \\frac{\\sum_{i=1}^{n}(y_i - \\overline{y})^2}{n-1}$$\n",
    "\n",
    "are sample variances of the regressor $x_j$ and $y$. All of the scaled regressors and the scaled responses have sample mean equal to zero and sample variane equal to 1. Using these new variables, the regression model becomes:\n",
    "\n",
    "$$w_i = b_1z_{i1} + b_2z_{i2} + ... + b_kz_{ik} + e_i, i=1,..., n.$$\n",
    "\n",
    "Centering the regressor and response variables removes the intercept from the model. The least-squares estimator of $b$ is:\n",
    "$$\\hat{b} = (Z'Z)^{-1}Z'w.$$\n",
    "\n",
    "\n",
    "**Unit Length Scaling**\n",
    "\n",
    "The second popular scaling is unit length scaling:\n",
    "\n",
    "$$w_{ij} = \\frac{x_{ij} - \\overline{x}_j}{s_{jj}^{1/2}}, \\quad i=1, ..., n, \\quad j=1,...,k,$$\n",
    "\n",
    "and \n",
    "\n",
    "$$y_i^0 = \\frac{y_i - \\overline{y}}{SS_T^{1/2}}, \\quad i=1,...,n,$$\n",
    "\n",
    "where\n",
    "\n",
    "$$S_{jj} = \\sum_{i=1}^{n}(x_{ij} - \\overline{x}_j)^2$$\n",
    "\n",
    "is the corrected sum of squares for regressor $x_j$. In this scaling, each new regressor $w_j$ has mean 0 and length 1. In terms of these variables, the regression model is:\n",
    "\n",
    "$$y_i^0 = b_1w_{i1} + b_2w_{i2} + b_kw_{ik} + e_i, \\quad i=1,2,...,n.$$\n",
    "\n",
    "The vector of least-squares regression coefficients is:\n",
    "$$\\hat{b} = (W'W)^{-1}W'y^0.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10. Multicollinearity\n",
    "\n",
    "Regression models are used for a wide variety of applications. A serious problem that may dramatically impact the usefulness of a regression model is multicollinear- ity, or near-linear dependence among the regression variables. In this section we briefly introduce the problem and point out some of the harmful effects of multicollinearity. A more extensive presentation, including more information on diagnostics and remedial measures, is in Chapter 9.\n",
    "\n",
    "Multicollinearity implies near-linear dependence among the regressors. The regressors are the columns of the $X$ matrix, so clearly an exact linear dependence would result in a singular $X'X$.\n",
    "\n",
    "In the soft drink delivery time data the variances of the regression coefficients are inflated because of the multicollinearity. This multicollinearity is evident from the nonzero off-diagonal elements in $W'W$. These off-diagonal elements are usually called simple correlations between the regressors, although the term correlation may not be appropriate unless the $x$’s are random variables. The off-diagonals do provide a measure of linear dependency between regressors. Thus, multicollinearity can seriously affect the precision with which regression coefficients are estimated.\n",
    "\n",
    "The main diagonal elements of the inverse of the $X'X$ matrix in correlation form [$(W'W)−1$ above] are often called variance inflation factors (VIFs), and they are an important multicollinearity diagnostic.\n",
    "\n",
    "We can show that, in general, the VIF for the $j$-th regression coefficient can be written as:\n",
    "$$VIF_j = \\frac{1}{1-R_j^2},$$\n",
    "where $R_j^2$ is the coefficient of mulitple determination obtained from regressing $x_j$ on the other regressor variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.11 Why do regression coefficient have the wrong sign?\n",
    "\n",
    "When using multiple regression, occasionally we find an apparent contradiction of intuition or theory when one or more of the regression coefficients seem to have the wrong sign. For example, the problem situation may imply that a particular regression coefficient should be positive, while the actual estimate of the parameter is negative. This “wrong”-sign problem can be disconcerting, as it is usually difficult to explain a negative estimate (say) of a parameter to the model user when that user believes that the coefficient should be positive. Mullet [1976] points out that regression coefficients may have the wrong sign for the following reasons:\n",
    "1. The range of some of the regressors is too small.\n",
    "2. Important regressors have not been included in the model. \n",
    "3. Multicollinearity is present.\n",
    "4. Computational errors have been made.\n",
    "\n",
    "In the simple linear regression, the variance of the regression coefficient $\\hat{b}_1$ is \n",
    "$$Var(b_1) = \\frac{\\sigma^2}{\\sum (x_i - \\overline{x})^2}.$$\n",
    "\n",
    "Note that the variance of $\\hat{b}_1$ is inversely proportional to the spread of the regressor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
